---
title: "Models"
author: "Mangaraj & Saha" 
output: 
  html_document:
    toc: true
    toc_depth: 4 
    toc_float:
      collapsed: false
      smooth_scroll: false 
--- 

## Introduction 
After exploring the data and preprocessing it, we have one file "imputed.rds" which has no missing values and is fit to train models. <br /> 
In this section we train three classification models using the data. <br /> 

## Decision Tree 

```{r  echo=FALSE, warning=FALSE, message=FALSE}
pkg <- c("knitr")
new.pkg <- pkg[!(pkg %in% installed.packages())]

#if It is not previously installed, install it. 
if (length(new.pkg)) {
  install.packages(new.pkg, repos = "http://cran.rstudio.com")
}

library(knitr)
read_chunk(path="decision_tree.R")  
```
We partition the data to training (80%) and test data (20%). <br /> 

```{r echo=FALSE, warning=FALSE, message=FALSE}
<<InstallPackages>> 
<<LoadPackages>> 
<<LoadDataset>> 
<<BeginDecisionTree>> 
<<DTRpartPackage>> 
```
Creating the decision tree. <br /> 
Class models: display the classification rate at the node, expressed as the number of correct classifications and the number of observations in the node.
```{r echo=FALSE, warning=FALSE, message=FALSE}
rpart.plot(dt1_rpart,extra = 2)
```

Class models: misclassification rate at the node, expressed as the number of incorrect classifications and the number of observations in the node.
```{r echo=FALSE, warning=FALSE, message=FALSE}
rpart.plot(dt1_rpart,extra = 3)
```

Class models: the probability of the fitted class.
```{r echo=FALSE, warning=FALSE, message=FALSE}
rpart.plot(dt1_rpart,extra = 8)
```
 
The summary gives the complete information about the tree,

* The number of nodes, information regarding each node,
* Information regarding each split and split conditions,
* Class counts over the data as well as the probabilities of classes at eah node.
* It also gives info about primary split and surrogate splits
A primary splitting rule : is always calculated by default, and it provides for the assignment of observations when the predictor variable is missing, even when there are no missing values in the training data. This allows for the possibility of missing values when you are scoring new data
A surrogate split : The purpose of a surrogate rule is to handle the assignment of observations by using an alternative variable that has similar predictive ability and has nonmissing values in observations where the primary predictor is missing. 
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<rpartSummary>> 
```

**Examining the complexity of the plot** <br />
This plots the various cp values.provides a graphical representation to the cross validated error summary. The cp values are plotted against the geometric mean to depict the deviation until the minimum value is reached.<br />
The value of cp should be least, so that the cross-validated error rate is minimum.
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<ExamineComplexity>> 
plotcp(dt1_rpart)
```

**Computing Accuracy of the rpart tree:**<br />
```{r echo=FALSE, warning=FALSE, message=FALSE}
base_accuracy
```

**Grow a tree with minsplit of 500 and max depth of 10**<br />
```{r echo=FALSE, warning=FALSE, message=FALSE}
rpart.plot(dt_rpart_preprun)
```


**Checking the accuracy of this approach:**<br />
```{r echo=FALSE, warning=FALSE, message=FALSE}
accuracy_preprun
```

**After Pruning:**<br />
```{r echo=FALSE, warning=FALSE, message=FALSE}
data.frame(base_accuracy, accuracy_preprun, accuracy_postprun)
```

**Decision Trees using selected features:**<br />
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<DTBorutaFeatures>> 
rpart.plot(dt2_rpart,extra = 4)
```

The summary gives the complete information about the tree, the number of nodes, information regarding each node, information regarding each split and split conditions, Class counts over the data as well as the probabilities of classes at eah node. It also gives info about primary split and surrogate splits. A primary split and surrogate split. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
<<DTBorutaSummary>> 
summary(dt2_rpart)
```

**Cross validated error summary:**
```{r echo=FALSE, warning=FALSE, message=FALSE}
plotcp(dt2_rpart)
```

**Accuracy:**
```{r echo=FALSE, warning=FALSE, message=FALSE}
base_accuracy2
```

**Growing a tree with minSplit 200 and MaxDepth 10:** 
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<DTBoruta200d10>> 
rpart.plot(dt2_rpart) 
```
**Accuracy:**
```{r echo=FALSE, warning=FALSE, message=FALSE}
accuracy2
```

**Growing a tree with minSplit 100 and MaxDepth 10:** 
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<DTBoruta100d10>> 
rpart.plot(dt3_rpart)
```
**Accuracy:**
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<DTBorutaAccu>> 
accuracy3
```


**DT with rpart package with Important Features from Random Forest:** 
```{r echo=FALSE, warning=FALSE, message=FALSE}

<<DTRPartRanForP
>> 

  rpart.plot(dt3_rpart,extra = 4)
```
**Examining Complexity:**
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<DTExamComplePlot>> 
plotcp(dt3_rpart)
```
**Accuracy:**
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<DTAccuracyRpartTree>> 
base_accuracy3
```

**Grow a tree with minsplit of 200 and max depth of 10**
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<DTMinSplit200D10>> 
rpart.plot(dt3_rpart_pre)
```
**Accuracy:**
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<DTAccurPrunedTree>> 
accuracy_pre3
```

**Grow a tree with minsplit of 100 and max depth of 10**
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<DTRPartRanFor>> 
rpart.plot(dt3_rpart_post)
```
**Accuracy:**
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<DTRPartRanForAccur>> 
accuracy_post3
```
**After Pruning:**<br />
```{r echo=FALSE, warning=FALSE, message=FALSE}
data.frame(base_accuracy3, accuracy_pre3, accuracy_post3)
```

**Tabulation of work so far:**
```{r echo=FALSE, warning=FALSE, message=FALSE}
df_acc_2
<<BeginEval>> 
```

### Evaluation
The decision tree with selected features using Boruta method has given better results. 
```{r echo=FALSE, warning=FALSE, message=FALSE}
ggplot(df_acc_2) + geom_density(aes(Base_Acc, color = Model_Name)) + 
  geom_jitter(aes(Base_Acc, 0, color = Model_Name), alpha = 0.5, height = 0.02 )

barplot(df_acc_2$Base_Acc, main="Decision Tree analysis", xlab="Model Names", ylab="Base Accuracy", 
        names.arg=c("All","Bor","RF"),
        border="red", density=c(0.8258, 0.8988, 0.8442))
```











 
 





## Random Forest 
```{r  echo=FALSE, warning=FALSE, message=FALSE}
pkg <- c("knitr")
new.pkg <- pkg[!(pkg %in% installed.packages())]

#if It is not previously installed, install it. 
if (length(new.pkg)) {
  install.packages(new.pkg, repos = "http://cran.rstudio.com")
}

library(knitr)
read_chunk(path="ran_for.R")  
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
<<InstallPackages>>
<<LoadPackages>>
<<LoadDataset>> 
<<ModelAlgoRandomForest>>
```
We are dividing the dataset into two parts:

* Training Data (80%)
* Test Data (20%)

We will implement Random Forest Model in 4 ways: <br />

* Default values of MTry and NTree
* Run 10 times, for different values of NTrees and analyse the Out of Bag (OOB) errors.
* Run 10 times, for different values of MTry and analyse the OOB errors. 
* Run for optimum number of MTry and NTrees where error is minimised and stabilises. 

### Approach 1: Default values of MTry and NTree
<center> 
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<defMTreeNTreeVal>>  
plot(rf1.cv, type = "cv", main = "CV producers accuracy")
```
<b>Fig: Cross Validation Producers Accuracy.</b><br />
```{r echo=FALSE, warning=FALSE, message=FALSE}
plot(rf1.cv, type = "model", main = "Model producers accuracy")
```
<b>Fig: Model Producers Accuracy.</b><br />
```{r echo=FALSE, warning=FALSE, message=FALSE}
plot(rf1.cv, type = "cv", stat = "oob", main = "CV oob error")
```
<b>Fig: Cross Validation vs Model OOB.</b><br />
```{r echo=FALSE, warning=FALSE, message=FALSE}
plot(rf1.cv, type = "model", stat = "oob", main = "Model oob error")
```
<b>Fig: Model OOB Error.</b><br />
</center>

#### Test results.
When tested with the test data (20% of the dataset), this was the result:
```{r echo=FALSE, warning=FALSE, message=FALSE} 
<<Predict>>
confusionMatrix(p2_rf,train_rf$liver_fat)
```

```{r echo=FALSE, warning=FALSE, message=FALSE} 
plot(rf1)
```
<center><b> Fig: Error Rate for Random Forest Model approach 1. <br /></center> 

### Approach 2: Optimising MTry. 
The number of variables  available for splitting at each tree node, in the random forests literature, is referred to as the mtry parameter.
<b>How does optimising MTrys affect? </b> 

While trying to optimise MTrys, we found out the following: 
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<defMTryNTreeVal2>> 
  print(mtry)
```
The best MTry which has minimum OOB Error is - 
```{r echo=FALSE, warning=FALSE, message=FALSE} 
  print(best_mtry)
```

#### Test Results.
When tested with the testing data:
```{r echo=FALSE, warning=FALSE, message=FALSE}
print(p1_rf_mtry)
``` 

### Approach 3: Optimisng NTrees. 
N Trees refer to the number of decision trees we are building. Larger number of trees produce more stable models and covariate importance estimates, but require more memory and a longer run time. When we try to find out the optimum number of N Trees, we got this: 
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<defMTryNTreeVal3>> 
  summary(results)
```

#### Test Results.
When tested with the testing data, we got the following: 
```{r echo=FALSE, warning=FALSE, message=FALSE}
plot(rf1_ntree)
```

### Approach 4: Optimisnig NTrees and MTrys to minimise error.
In this approach, we take the optimised MTrys and NTrees to find the best model. 

```{r echo=FALSE, warning=FALSE, message=FALSE} 
<<defMTryNTreeVal4>> 
  rf1_op$confusion
```

We then do cross validation with 0 fold. The following are the plots of cross validation verses model producers accuracy:

```{r echo=FALSE, warning=FALSE, message=FALSE} 
plot(rf1_op.cv, type = "cv", main = "CV producers accuracy")
plot(rf1_op.cv, type = "model", main = "Model producers accuracy")
```

#### Test Results.
When we tested it with the test data, we got the following result:
```{r echo=FALSE, warning=FALSE, message=FALSE} 
plot(rf1_op)
```

## Linear Regression 
```{r  echo=FALSE, warning=FALSE, message=FALSE} 
read_chunk(path="linear_regression.R")  
```
Now we try and implement the Linear Regression model on the data. <br /> 
We load the dataset, choose the features selected by the Boruta Method and the random forest method. <br /> 
We further divide the dataset to train and test data. <br /> 

```{r echo=FALSE, warning=FALSE, message=FALSE}
<<InstallPackages>>
<<LoadPackages>>
<<LoadDataset>>  
<<ProcessData>> 
```
We would be implementing multiple linear regression (MLR): <br />
1. MLR with all attributes , check p-Value, AIC, BIC <br />
2. MLR with Important attributes from Random Forest <br /> 
3. MLR with Boruta features <br /> 

<br />
To visualize linear relationship between dependant and independant variables we are creating the following scatter plots. <br />
Here we tried to plot scatter plots to see the reltionship of various individual predictors with the target variable. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
<<furtherSteps>>
```
<br /> 
Here we plot a density curve to see the distribution od data of target variable (Liver_fat)<br /> 
Inference: Maximum people have a liver fat value les than 10 which is the threshhold limit. <br /> 
Any one having liver fat value more than 10 is considered to have fatty liver.
This curve helps to see the data districution. <br /> 

```{r echo=FALSE, warning=FALSE, message=FALSE}
ggplot(train_lin, aes(liver_fat)) + geom_density(fill="blue")
```
<br />
Density Graph: To check how much is the response variable close to normality
For som_bmi_s0, the graph is right skewed (positive skew)<br />

```{r echo=FALSE, warning=FALSE, message=FALSE}
<<furtherStepsDensityG>>
```
<br /> 
For ferri_s0 the graph is right skewed(positive skew)
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<furtherStepsDensityGferri>> 
<<MLR1>> 
```

### MLR-1
MLR with all attributes , check p-Value, AIC, BIC. <br />
Summary of the model:
```{r echo=FALSE, warning=FALSE, message=FALSE}
summary(model1)
```
The Linear Regression Model:
```{r echo=FALSE, warning=FALSE, message=FALSE}
plot(model1)
```

#### Observations:
1. Residuals vs Fitted <br />
There could be a non-linear relationship between predictor variables and an 
outcome variable and the pattern could show us in this plot. With the plot for model1 we could infer that the relationship is almost linear. 
<br />
2. Normal Q-Q<br />
This plot shows if residuals are normally distributed and do residuals follow 
a straight line well or do they deviate severely. 
The residuals are lined well on the straight dashed line for model1 which is good.
<br />
3. Scale-Location<br />
This plot shows if residuals are spread equally along the ranges of predictors.
Used to check the assumption of equal variance.
It's good to have a horizontal line with equally (randomly) spread points.
Model1 gives almost a horizontal line. <br />
4. Residuals vs Leverage<br />
This plot helps us to find influential cases if any. i.e. see if there is any infuential outlier in the data that affets the linear regression. <br />
For model1 we dont see any specific infuential outlier. <br />

Confidence Interval:
```{r echo=FALSE, warning=FALSE, message=FALSE}
confint(model1)
```

#### Model comparision parameters
AIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model,so that a lower AIC means a model is considered to be closer to the truth.

```{r echo=FALSE, warning=FALSE, message=FALSE}
AIC (model1)
```

BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model.
```{r echo=FALSE, warning=FALSE, message=FALSE}
BIC(model1)
```

#### Graph plots 
Plots to see how the model has fitted the data for certain features.<br /> 
Grey area is the confidence region. <br />
1. age_ship_s0
```{r echo=FALSE, warning=FALSE, message=FALSE}
ggplot(data = train_lin, aes(x = age_ship_s0, y = liver_fat)) + geom_point()  +
  stat_smooth(method = "lm", col = "dodgerblue3") +
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Linear Model Fitted to Data")
```

2. som_bmi_s0
```{r echo=FALSE, warning=FALSE, message=FALSE}
ggplot(data = train_lin, aes(x = som_bmi_s0, y = liver_fat)) + geom_point()  +
  stat_smooth(method = "lm", col = "dodgerblue3") +
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Linear Model Fitted to Data")
```

### MLR-2
The linear regression model is created, taking into consideration the important features colleced from varImp function of Random Forest. <br /> 
The summary function gives the performance summary of the model such as:Residuals, coeficients(intercept and slope for each feature), t-Value, residual standard error, Multiple R Squared value, F statistics. <br /> 

```{r echo=FALSE, warning=FALSE, message=FALSE}
<<MLR2>>
summary(model2)
```

Plotting the model2 (MLR-2):
```{r echo=FALSE, warning=FALSE, message=FALSE}
plot(model2)
```

#### Observations:
Residual vs Fitter is almost linear. Normal Q-Q has lined up well with the dashed line but not as perfect as model1. Scale-Location line is almost horizontal. Residual vs Leverage line touches some important outliers. 
<br /> 
Confidence interval: 
```{r echo=FALSE, warning=FALSE, message=FALSE}
confint(model2)
```

#### Model Comparison Parameters.
AIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model,so that a lower AIC means a model is considered to be closer to the truth.

```{r echo=FALSE, warning=FALSE, message=FALSE}
AIC (model2)
```
BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model.

```{r echo=FALSE, warning=FALSE, message=FALSE}
BIC(model2)
```

### MLR-3
In this approach, we use all the features selected by the Boruta Method. 
<br />
The summary function gives the performance summary of the model such as:Residuals, coeficients(intercept and slope for each feature), t-Value, Residual standard error, Multiple R Squared value, F statistics
```{r echo=FALSE, warning=FALSE, message=FALSE}
<<MLR3>>
summary(model3)
```

Plotting Model3 (MLR-3)
```{r echo=FALSE, warning=FALSE, message=FALSE}
plot(model3)
```

#### Observations: 
Residual vs Fitter is almost linear. <br />
Normal Q-Q has lined up well with the dashed line but not as perfect as model1. <br />
Scale-Location line is almost horizontal.<br />
Residual vs Leverage line touches some important outliers.<br />

Confidence Interval: 
```{r echo=FALSE, warning=FALSE, message=FALSE}
confint(model3)
```
AIC Value: 
```{r echo=FALSE, warning=FALSE, message=FALSE}
AIC (model3)
```
BIC Value: 
```{r echo=FALSE, warning=FALSE, message=FALSE}
BIC(model3)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
<<AnalyVariance>> 
```
### Analysis of Variances. 
In this section we will try and compare the different multiple linear regression approaches: 

#### Model 1 vs Model 2
```{r echo=FALSE, warning=FALSE, message=FALSE}
anova(model1, model2)
```
Observation: Model1 is proving better for its lower value fo RSS and other attributes. 

#### Model 2 vs Model 3
```{r echo=FALSE, warning=FALSE, message=FALSE}
anova(model2, model3)
```
Observation: Almost similar, but Model 3 yields slightly better. 

#### Model 1 vs Model 3
```{r echo=FALSE, warning=FALSE, message=FALSE}
anova(model3, model1)
```
Observation: Model 1 is better than Model 3. 


#### Plots: 
Plot1 showing comparision of 3 models. <br /> 
The more the R square value, better the model. 
```{r echo=FALSE, warning=FALSE, message=FALSE}
ggplot(df_acc_l) + geom_density(aes(RSq, color = Model_Name)) + 
  geom_jitter(aes(RSq, 0, color = Model_Name), alpha = 0.5, height = 0.02 )
```

Plot2 showing comparision of 3 models. The taller the height of the bar of a model, the better it is. 
```{r echo=FALSE, warning=FALSE, message=FALSE}
barplot(df_acc_l$RSq, main="Linear Regression analysis with RSquared", xlab="Model Names", ylab="Accuracy", 
        names.arg=c("M1","M2","M3"),
        border="blue", density=c(85.96, 56.95, 57.04))
```

Histogram to visualize model1
If the plot is symmetrical around 0 that means the model has fit the data well
```{r echo=FALSE, warning=FALSE, message=FALSE}
ggplot(data=train_lin, aes(model1$residuals)) + 
  geom_histogram(binwidth = 1, color = "black", fill = "purple4") +
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Histogram for Model Residuals")
```

Histogram to visualize model2
```{r echo=FALSE, warning=FALSE, message=FALSE}
ggplot(data=train_lin, aes(model2$residuals)) + 
  geom_histogram(binwidth = 1, color = "black", fill = "green2") +
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Histogram for Model Residuals")
```

Histogram to visualize model3
```{r echo=FALSE, warning=FALSE, message=FALSE}
ggplot(data=train_lin, aes(model3$residuals)) + 
  geom_histogram(binwidth = 1, color = "black", fill = "blue 4") +
  theme(panel.background = element_rect(fill = "white"),
        axis.line.x=element_line(),
        axis.line.y=element_line()) +
  ggtitle("Histogram for Model Residuals")
```

Scatter plot of BMI_s1 vs. Liver Fat. <br /> 
```{r echo=FALSE, warning=FALSE, message=FALSE}
ggplot(data = train_lin, mapping = aes(x = som_bmi_s1, y = liver_fat)) + 
  geom_point() +
  geom_smooth(method = "lm") + 
  labs(x = "BMI_s1", y = "Liver_fat") +
  ggtitle(expression(atop("Scatterplot of BMI_s1 vs. liver_fat", atop(italic("With Fitted Regression Line", ""))))) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.subtitle = element_text(hjust = 0.5))
```

Accumulated Local Effects plot. 
```{r echo=FALSE, warning=FALSE, message=FALSE} 
ggplot(df_data_lin, aes(som_bmi_s0, liver_fat)) +
  geom_point() +
  geom_segment(data = as_tibble(intrvls), aes(x = value, xend = value), 
               y = min(df_data_lin$liver_fat) - 0.05 * (max(df_data_lin$liver_fat) - min(df_data_lin$liver_fat)), 
               yend = max(df_data_lin$liver_fat) + 0.05 * (max(df_data_lin$liver_fat) - min(df_data_lin$liver_fat)), linetype = 2) +
  geom_segment(data = df_data_lin, aes(x = intrvls[3], xend = intrvls[4], y = df_data_lin$liver_fat, yend = df_data_lin$liver_fat), 
               arrow = arrow(length = unit(2, "mm"), type = "closed", ends = "both"), color = "blue") +
  geom_text(data = tibble(id = 1:length(intrvls), int = intrvls), aes(x = int, y = 0.925, label = id))


```



